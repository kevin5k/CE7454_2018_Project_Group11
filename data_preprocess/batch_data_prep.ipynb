{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Necessary Support Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import os.path as osp\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils import data as utils_data\n",
    "import torch.nn.functional as Func\n",
    "from torch.autograd import Variable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import preprocess_lib as prelib  # Load Custom Library RLE_decode function\n",
    "from tqdm import tqdm  # For Progress Bar\n",
    "\n",
    "# import gc\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defines Global Parameters and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common path variables\n",
    "train_path = '../train/'\n",
    "test_path = '../test/'\n",
    "train_label_path = '../input/train_ship_segmentations.csv'\n",
    "test_label_path = '../input/test_ship_segmentations.csv'\n",
    "train_label_data = prelib.LoadMyData(train_label_path, pandas=True)\n",
    "\n",
    "# Global Variables\n",
    "testimg = 0\n",
    "batchsz = 1024  # Define Batch Size for DataLoader Import\n",
    "sample = 0  # Used for datasample loading\n",
    "sample_32 = 0  # Used for down-sampling and resizing of original sample data\n",
    "savetrainlabels = 0  # Boolean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supporting Classes and Functions defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the image - Pytorch Documentation Tutorial\n",
    "# https://discuss.pytorch.org/t/resizing-any-simple-direct-way/10316/6\n",
    "def resize2d(img, size):\n",
    "    return (Func.adaptive_avg_pool2d(Variable(img,volatile=True), size)).data\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "def imshow_2(img, mask, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    mask = mask.numpy().transpose((1, 2, 0))\n",
    "    mask = np.clip(mask, 0, 1)\n",
    "    fig = plt.figure(figsize = (6,6))\n",
    "    plt.imshow(mask_overlay(img, mask))\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)     \n",
    "    \n",
    "def imshow_unnorm(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbusDS(utils_data.Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader.\n",
    "    \"\"\"\n",
    "#     print(utils_data.Dataset)\n",
    "    def __init__(self, root, transform=None):\n",
    "        \"\"\" Intialize the dataset\n",
    "        \"\"\"\n",
    "        self.filenames = []\n",
    "        self.root = root\n",
    "        self.transform = transforms.ToTensor()  # original\n",
    "#         self.transform = transform  #mod\n",
    "        filenames = glob.glob(osp.join(train_path, '*.jpg'))\n",
    "        for fn in filenames:\n",
    "            self.filenames.append(fn)\n",
    "        self.len = len(self.filenames)\n",
    "        \n",
    "    # You must override __getitem__ and __len__\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"\n",
    "        image = Image.open(self.filenames[index])\n",
    "#         sample = self.transform(image)\n",
    "        return self.transform(image)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        landmarks = landmarks * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'landmarks': landmarks}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'landmarks': torch.from_numpy(landmarks)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate necessary dataset Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation with normalization\n",
    "normalized_dataset = AirbusDS(train_path, transform=transforms.Compose([\n",
    "    #Rescale(224), \n",
    "    #ToTensor(), \n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5], \n",
    "        std=[0.5, 0.5, 0.5])\n",
    "#         mean=[0.485, 0.456, 0.406], # This is the cifar10 mean and std\n",
    "#         std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    ") \n",
    "\n",
    "imgloader = utils_data.DataLoader(normalized_dataset, batch_size=batchsz,\n",
    "                            shuffle=True, num_workers=0)\n",
    "\n",
    "# Create only single sintance of data iterator to minimise memory leak. \n",
    "dataiter = iter(imgloader)\n",
    "\n",
    "# Added Progress Bar for better presentation\n",
    "for i_batch in tqdm(range(batchsz)):\n",
    "    sample = next(dataiter)  # Only iterate the data and pass to     \n",
    "\n",
    "sample_32 = resize2d(sample, (224,224)).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Show Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show images\n",
    "imshow(torchvision.utils.make_grid(sample))                                \n",
    "imshow(torchvision.utils.make_grid(sample_32))    \n",
    "print(sample.size())\n",
    "print(sample_32.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Respective batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Batch Data\n",
    "torch.save(sample,'../input/test_data_64bit_sz768-B01.pt')\n",
    "torch.save(sample_32,'../input/test_data_32bit_sz224-B01.pt')\n",
    "\n",
    "if savetrainlabels==True:\n",
    "    torch.save(train_label_data, '../input/train_label.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
